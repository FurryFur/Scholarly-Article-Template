<h1 data-label="148606" class="ltx_title_section">Model</h1><div></div><div>Our model builds heavily on the WaveGAN architecture by [Author names here][Reference here]. The original WaveGAN implementation ported the DCGAN [Reference to original DCGAN paper] architecture to 1D to work with audio data. We replace the existing 1D DCGAN model with our own model, heavily inspired by Progressive GAN [paper reference]. We also extend the model to support conditioning text, taking heavy inspiration from StackGAN's [paper reference] text conditioning implementation. StackGAN's model uses text embeddings, which are downsampled to 128 dimensions, however, the embedding model used is not specified. We choose to use ELMo [Reference] for our text embedding for the reasons stated in section (<span class="au-ref raw v1">\ref{151756}</span>).&nbsp;</div><div></div><div>We extend the original Progressive GAN model [reference] by replacing pairs of convolutions with residual blocks, containing two convolutions and one skip connection each (Fig.&nbsp;<span class="au-ref raw v1">\ref{801834}</span>) We use similar residual blocks to those used in the Improved Training of Wasserstein GANs paper [Reference] [figure]. We stick to the pre-activation scheme used in the paper as it has been previously shown to give better results than other activation schemes [Reference, see&nbsp;<a href="https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cc5d0adf648e">https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cc5d0adf648e</a>]. Our residual block scheme is shown in figure&nbsp;<span class="au-ref raw v1">\ref{801834}</span>.</div><div></div><div>We implement progressive growing by giving each up or down block an on</div>