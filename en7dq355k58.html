<h2 data-label="752902" class="ltx_title_subsection">Interpretability</h2><div>Our baseline interpretability on the real dataset comes out at 72% (Fig.&nbsp;<span class="au-ref raw v1">\ref{626565}</span>), while interpretability on the generated sound effects comes out at 62% (Fig.&nbsp;<span class="au-ref raw v1">\ref{576840}</span>). We note that these values are somewhat artificially decreased over what they should be, due to our conditioning text dataset containing several single keywords that do not, on their own, describe the sounds they are paired with. Examples are keywords like 'hard', 'low' and 'fast', which we observed frequently while performing the interpretability tests with judges. When these keywords appear, judges are forced to mark their guessed keywords as incorrect, even when it may be clear what the sound effect actually represents. However, since these keywords appear as true labels in both the real and generated interpretability tests, the comparison should still be fair. Our interpretability score for generated sound effects is fairly high, however, this may be due to low number modalities covered by our evaluation generator, it is fairly easy for judges to guess one of the six main elements from the magic dataset (ice, fire, earth, air, black and generic) and be correct most of the time, although judges were asked to be more specific in their labelling (e.g. 'ice hitting a hard surface and shattering'). We note that our model is very easily interpretable&nbsp;</div>