@article{Zhou_1988,
  doi = {10.1021/bi00414a027},
  url = {https://doi.org/10.1021%2Fbi00414a027},
  year = {1988},
  month = {jul},
  publisher = {American Chemical Society ({ACS})},
  volume = {27},
  number = {14},
  pages = {5129--5135},
  author = {Junmei Zhou and Zhixiong Xue and Ziyun Du and Teri Melese and Paul D. Boyer},
  title = {{Relationship of tightly bound {ADP} and {ATP} to control and catalysis by chloroplast {ATP} synthase}},
  journal = {Biochemistry},
}


@article{Boyer_1998,
  doi = {10.1002/(sici)1521-3773(19980918)37:17<2296::aid-anie2296>3.0.co;2-w},
  url = {https://doi.org/10.1002%2F%28sici%291521-3773%2819980918%2937%3A17%3C2296%3A%3Aaid-anie2296%3E3.0.co%3B2-w},
  year = {1998},
  month = {sep},
  publisher = {Wiley-Blackwell},
  volume = {37},
  number = {17},
  pages = {2296--2307},
  author = {Paul D. Boyer},
  title = {{Energy Life, and {ATP} (Nobel Lecture)}},
  journal = {Angewandte Chemie International Edition},
}


@article{28439448,
  title = {{Spatial Determinants of Ebola Virus Disease Risk for the West African Epidemic.}},
  date = {2017 Mar 31},
  source = {PLoS Curr},
  authors = {Zinszer, K and Morrison, K and Verma, A and Brownstein, JS},
  author = {Zinszer, K and Morrison, K and Verma, A and Brownstein, JS},
  year = {2017},
  month = {Mar},
  journal = {PLoS Curr},
  volume = {9},
  number = {},
  pages = {},
  pubmed_id = {28439448},
}


@article{zhang2016stackgan,
  title = {{Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. arXiv preprint}},
  author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Huang, Xiaolei and Wang, Xiaogang and Metaxas, Dimitris},
  journal = {arXiv preprint arXiv:1612.03242},
  volume = {2},
  number = {3},
  pages = {5},
  year = {2016},
}


@article{zhang2017style,
  title = {{Style transfer for anime sketches with enhanced residual u-net and auxiliary classifier gan}},
  author = {Zhang, Lvmin and Ji, Yi and Lin, Xin},
  journal = {arXiv preprint arXiv:1706.03319},
  year = {2017},
}


@article{karras2017progressive,
  title = {{Progressive growing of gans for improved quality, stability, and variation}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal = {arXiv preprint arXiv:1710.10196},
  year = {2017},
}


@article{hoang2017multi,
  title = {{Multi-Generator Gernerative Adversarial Nets}},
  author = {Hoang, Quan and Nguyen, Tu Dinh and Le, Trung and Phung, Dinh},
  journal = {arXiv preprint arXiv:1708.02556},
  year = {2017},
}


@article{ghosh2017multi,
  title = {{Multi-agent diverse generative adversarial networks}},
  author = {Ghosh, Arnab and Kulharia, Viveka and Namboodiri, Vinay and Torr, Philip HS and Dokania, Puneet K},
  journal = {CoRR, abs/1704.02906},
  volume = {6},
  pages = {7},
  year = {2017},
}


@article{Park2018-ez,
  title = {{{MEGAN}: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation}},
  author = {Park, David Keetae and Yoo, Seungjoo and Bahng, Hyojin and Choo, Jaegul and Park, Noseong},
  abstract = {Recently, generative adversarial networks (GANs) have shown promising performance in generating realistic images. However, they often struggle in learning complex underlying modalities in a given dataset, resulting in poor-quality generated images. To mitigate this problem, we present a novel approach called mixture of experts GAN (MEGAN), an ensemble approach of multiple generator networks. Each generator network in MEGAN specializes in generating images with a particular subset of modalities, e.g., an image class. Instead of incorporating a separate step of handcrafted clustering of multiple modalities, our proposed model is trained through an end-to-end learning of multiple generators via gating networks, which is responsible for choosing the appropriate generator network for a given condition. We adopt the categorical reparameterization trick for a categorical decision to be made in selecting a generator while maintaining the flow of the gradients. We demonstrate that individual generators learn different and salient subparts of the data and achieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA and a competitive unsupervised inception score of 8.33 in CIFAR-10.},
  month = {may},
  year = {2018},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV},
  eprint = {1805.02481},
}


@article{Thanh-Tung2018-ev,
  title = {{On catastrophic forgetting and mode collapse in Generative Adversarial Networks}},
  author = {Thanh-Tung, Hoang and Tran, Truyen and Venkatesh, Svetha},
  abstract = {Generative Adversarial Networks (GAN) are one of the most prominent tools for learning complicated distributions. However, problems such as mode collapse and catastrophic forgetting, prevent GAN from learning the target distribution. These problems are usually studied independently from each other. In this paper, we show that both problems are present in GAN and their combined effect makes the training of GAN unstable. We also show that methods such as gradient penalties and momentum based optimizers can improve the stability of GAN by effectively preventing these problems from happening. Finally, we study a mechanism for mode collapse to occur and propagate in feedforward neural networks.},
  month = {jul},
  year = {2018},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  eprint = {1807.04015},
}


@article{Gulrajani2017-xz,
  title = {{Improved Training of Wasserstein {GANs}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  month = {mar},
  year = {2017},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  eprint = {1704.00028},
}
