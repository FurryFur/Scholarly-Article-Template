<h1 data-label="116600" class="ltx_title_section">Introduction</h1><div></div><div></div><h1 data-label="961105" class="ltx_title_section">Existing Research&nbsp; &nbsp;</h1><div>Several recent papers such as&nbsp;Progressive GAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite> have found it beneficial to split the generative problem into smaller, easier to solve generative problems, that then build up to solve a harder generative problem. Progressive GAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite>     starts by training a very shallow network to generate low resolution images, then gradually adds more stages to the existing model, to generate higher, and higher resolution images. The key here is training different parts of the network to solve different problems. All the stages of Progressive GAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite>     are explicitly trained to generate images of a specific resolution, from images of a lower resolution. This is a much simpler problem than going directly from a low dimensional latent vector to a high dimensional output space. By training a network to solve small sub problems, that can be combined to solve a larger problem, the network is able to learn to generate extremely convincing results, even for a very high dimensional output space.</div><div></div><div>Vanishing gradients, Mode collapse and catastrophic forgetting are common problems plaguing current GAN architectures<cite class="ltx_cite raw v1">\cite{Thanh-Tung2018-ev}</cite>.&nbsp;</div><div></div><div> Splitting up the generative problem into multiple, easier to solve problems, has also been used to address the mode collapse problem&nbsp;<cite class="ltx_cite raw v1">\cite{hoang2017multi}</cite>,&nbsp;<cite class="ltx_cite raw v1">\cite{Park2018-ez}</cite>,&nbsp;<cite class="ltx_cite raw v1">\cite{ghosh2017multi}</cite>. These networks both use very similar architectures. The basic idea is to use multiple generators, instead of a single generator. Each generator is trained to produce a specific modality from the dataset. This is done by having the discriminator try to classify both whether the image is real or not, and what generator produced the image. In this way, each generator is trained to diversify itself from other generators, such that the discriminator is easily able to tell them apart. In order to reduce complexity these networks often use weight sharing between the different generators (usually all the weights are shared except in the final layers), although the authors on MADGAN note that not using weight sharing allows for generating from more diverse datasets.</div><div></div><div><a href="https://arxiv.org/pdf/1710.10916.pdf" target="_blank">StackGAN++</a>&nbsp;also splits the generative problem up into mutiple pipeline stages, similar to ProgressiveGAN. However StackGAN++ trains the entire pipeline at once, rather than progressively adding stages as one stage converges. StackGAN is of particular interest, as they are solving a text-to-image problem, which is similar to our text-to-sfx problem. StackGAN++ splits the generative problem into two different ones, generating realistic looking images, and generating images that match a specific condition. The discriminator in this network splits in two at the final layer, with one path mixing in the condition, and outputs a score for real images and matching conditions. In this way the discriminator is explicitly trained to tell if a generated image belongs to the category it was conditioned on. However, unlike MADGAN, it is unable to tell specifically what modality the generated image belongs to. If multiple generators are used, as in MADGAN, this is likely to cause the generators to all learn the same output distribution which maximally fools the discriminator, rather than diversifying into multiple modalities.</div><div>    </div>