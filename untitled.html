<h1 data-label="116600" class="ltx_title_section">Introduction</h1><div></div><div></div><h1 data-label="961105" class="ltx_title_section">Existing Research&nbsp; &nbsp;</h1><div>Several recent papers such as&nbsp;ProgressiveGAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite> have found it beneficial to split the generative problem into smaller, easier to solve generative problems, that then build up to solve a harder generative problem. ProgressiveGAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite>     starts by training a very shallow network to generate low resolution images, then gradually adds more stages to the existing model, to generate higher, and higher resolution images. The key here is training different parts of the network to solve different problems. All the stages of ProgressiveGAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite>     are explicitly trained to generate images of a specific resolution, from images of a lower resolution. This is a much simpler problem than going directly from a low dimensional latent vector to a high dimensional output space. By training a network to solve small sub problems, that can be combined to solve a larger problem, the network is able to learn to generate extremely convincing results, even for a very high dimensional output space.</div><div></div><div>Splitting up the generative problem into multiple, easier to solve problems, has also been used to solve the mode collapse problem in both&nbsp;MGAN<cite class="ltx_cite raw v1">\cite{hoang2017multi}</cite>&nbsp;and&nbsp;MADGAN(    </div><div>    </div>