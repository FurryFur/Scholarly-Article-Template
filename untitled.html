<h1 data-label="116600" class="ltx_title_section">Introduction</h1><div>Sound engineering for games is an important and challenging task for game developers. Good sound effects and atmospheric music can make a game a unique and memorable experience. However, sound effects are often neglected, and left to the last minute when working in small indie teams. The primary reason is due to not having a dedicated sound engineer available. Unfortunately a dedicated sound engineer is out of reach for many small indie teams with limited budgets. For non-experts, the the task of creating, finding and modifying sound effects to fit a particular game scenario can be very difficult and can have unsatisfactory results. For this reason sound effect generation tools like Bfxr[reference] are invaluable tools for indie developers. Unfortunately, these tools only work for a small subset of games genres, namely the 8 bit pixel genre, as they are only able to generate very simplistic sound effects. The inability to produce complex sound effects comes from these tools reliance on old procedural generation methods. In these tools basic sound waves (sin, sawtooth, square, etc) are used as a base, then combinations of various effects and modifiers such as attack time, sustain time, compression, frequency slide, and flanger sweep, are applied to produce the final sound effect. Variations of these sound effects can then created by randomly perturbing the modifier parameters to produce different sounds. We observe that machine learning research has come a very long way in the last few years. In particular, several generative models have recently succeeded in producing output that is nearly indistinguishable from the real thing[references]. In this work we apply these recent advances in machine learning to the task of sound effect generation for games. Our approach is based on generative adversarial networks, which have recently had success in generating photo-realistic images at resolutions up to 1 mega pixel[reference]. These models can only produce fixed size output, however, this is a good fit for sound effect generation where most individual sound events are less than 1 second in length. Generation of audio samples of arbitrary length is left as an open area for future research.&nbsp;</div><div></div><h1 data-label="961105" class="ltx_title_section">Existing Research&nbsp; &nbsp;</h1><div>    </div><div>Generative Adversarial Networks (GANs) are most well known for being able to produce highly realistic images. In particular they are very good at synthesizing convincing fine grained texture detail in comparison to other generative models. However, GANs are notoriously difficult to train and will often fail to converge. Vanishing gradients, mode collapse and catastrophic forgetting are all common problems plaguing the current GAN architectures<cite class="ltx_cite raw v1">\cite{Thanh-Tung2018-ev}</cite>. The current state of the art method for addressing these issues is to use the improved Wasserstein GAN training metric (WGAN-GP)<cite class="ltx_cite raw v1">\cite{Gulrajani2017-xz}</cite>. This metric works very well to ensure training stability. However, when trained on multi-modal datasets, WGAN-GP can still fail to converge due to the nature of the training metric pushing generated samples towards random real samples. This causes the generator to oscillate generated samples between different modalities and never converge<cite class="ltx_cite raw v1">\cite{Thanh-Tung2018-ev}</cite>.&nbsp;</div><div></div><div>Several recent papers such as&nbsp;Progressive GAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite> have found it beneficial to split the generative problem into smaller, easier to solve generative problems, that then build up to solve a harder generative problem. Progressive GAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite>     starts by training a very shallow network to generate low resolution images, then gradually adds more stages to the existing model, to generate higher, and higher resolution images. The key here is training different parts of the network to solve different problems. All the stages of Progressive GAN<cite class="ltx_cite raw v1">\cite{karras2017progressive}</cite>     are explicitly trained to generate images of a specific resolution, from images of a lower resolution. This is a much simpler problem than going directly from a low dimensional latent vector to a high dimensional output space. By training a network to solve small sub problems, that can be combined to solve a larger problem, the network is able to learn to generate extremely convincing results, even for a very high dimensional output space.</div><div></div><div> Splitting up the generative problem into multiple, easier to solve problems, has also been used to address the mode collapse problem&nbsp;<cite class="ltx_cite raw v1">\cite{hoang2017multi}</cite>,&nbsp;<cite class="ltx_cite raw v1">\cite{Park2018-ez}</cite>,&nbsp;<cite class="ltx_cite raw v1">\cite{ghosh2017multi}</cite>. These networks both use very similar architectures. The basic idea is to use multiple generators, instead of a single generator. Each generator is trained to produce a specific modality from the dataset. This is done by having the discriminator try to classify both whether the image is real or not, and what generator produced the image. In this way, each generator is trained to diversify itself from other generators, such that the discriminator is easily able to tell them apart. In order to reduce complexity these networks often use weight sharing between the different generators (usually all the weights are shared except in the final layers), although the authors on MADGAN note that not using weight sharing allows for generating from more diverse datasets.</div><div></div><div><a href="https://arxiv.org/pdf/1710.10916.pdf" target="_blank">StackGAN++</a>&nbsp;also splits the generative problem up into mutiple pipeline stages, similar to ProgressiveGAN. However StackGAN++ trains the entire pipeline at once, rather than progressively adding stages as one stage converges. StackGAN is of particular interest, as they are solving a text-to-image problem, which is similar to our text-to-sfx problem. StackGAN++ splits the generative problem into two different ones, generating realistic looking images, and generating images that match a specific condition. The discriminator in this network splits in two at the final layer, with one path mixing in the condition, and outputs a score for real images and matching conditions. In this way the discriminator is explicitly trained to tell if a generated image belongs to the category it was conditioned on. However, unlike MADGAN, it is unable to tell specifically what modality the generated image belongs to. If multiple generators are used, as in MADGAN, this is likely to cause the generators to all learn the same output distribution which maximally fools the discriminator, rather than diversifying into multiple modalities.</div><div>    </div>