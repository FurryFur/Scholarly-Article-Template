<h1 data-label="687807" class="ltx_title_section">Future Research</h1><h2 data-label="865239" class="ltx_title_subsection">Failure Cases</h2><div>We observe a number of failure cases in our present model which would be interesting areas for future research. Our first observation is that our present model fails to capture certain time dependent structures and long range dependencies. These include long range frequency changes, such as those typically present in power-up sounds, where the pitch increases or decreases with time [compare generated to real waveforms], as well as periodic or pulsating signals [compare generated to real waveforms], which require the generator to create multiple copies of the same audio signal positioned at different locations in the final output signal. In future we would like to expand our current model, or a variant of it, to a recurrent architecture, which is able to capture long range time dependencies and better model time dependent structure. This would also allow us to generate audio of varying length, instead of being limited to fixed length output. As it has been previously observed that the common LSTM and GRU recurrent networks have difficulty recreating accurate copies of data [Neural Turing Machine reference], a model with explicit memory such as a Neural Turing Machine (NTM) [Reference] or the more modern Differentiable Neural Computer (DNC) [Reference] could be of interest here in order to generate the repeated structures that are often found in audio data. We theorize that it could be possible for a typical GAN network, such as the one we have presented here, to generate the base waveforms required for the audio signal. These structures could then be stored in explicit memory inside a DNCs or NTMs memory, and written out by the controller network at the appropriate locations in the output signal. We prefer a GAN approach to training over explicitly modelling the loss as the error on the training signal, as GANs are able to generate individual samples from a plausible set, while explicit loss models are indecisive and tend to generate the average over a set of plausible samples [GAN super resolution paper reference]. For images, this results in the typical blurry output seen in many generative models. Adverserial training has&nbsp;</div><div></div><div>&nbsp;[To check: keyboard typing - gaps between audio events]. </div><div></div><div>Our model also has trouble generating layered sound effects. The presence of layered signals in audio data presents an interesting distinction from the image data that GANs are typically trained to generate. Image data typically contains less layered signals than audio data. Although image data can contain layered signals where the image contains reflections and transparency, there are typically only two or three signal layers in these cases, whereas audio data typically contains many overlapping signals. Sound effects especially are typically built up by layering different sounds on top of one another to produce rich and expressive audio signals. We find that our generative model has a tendency to produce somewhat muddy audio signals, which do not contain clearly distinguishable audio layers. An interesting area of future research would be to try to create a network that can produce a varying number of outputs, with each output corresponding to a separate audio source signal. These audio signals would then be combined to produce the final output signal. In order to do this we would first need to perform blind signal separation on the training data in order to separate out individual source signals from the original mixed signals. Then we would train a network to produce the set of source signals, instead of the final mixed signal.</div><div></div><div></div>