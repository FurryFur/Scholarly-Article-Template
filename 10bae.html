<h1 data-label="687807" class="ltx_title_section">Future Research</h1><div>We observe a number of failure cases in our present model which would be interesting areas for future research. Our first observation is that our present model fails to capture certain time dependent structures and long range dependencies. These include long range frequency changes, such as those typically present in power-up sounds, where the pitch increases or decreases as a function of time[compare generated to real waveforms], as well as periodic or pulsating signals [compare generated to real waveforms], which require the generator to create multiple copies of the same audio signal positioned at different locations throughout the final output signal. In future, we would like to expand our current model, or a variant of it, to a recurrent architecture, which is able to capture these long range dependencies and better model time dependent structure. This would also allow us to generate audio of varying length, instead of being limited to fixed length output. We note that it has been previously observed that the common LSTM and GRU recurrent networks have difficulty reliably repeating patterns in output signal over long time periods[Neural Turing Machine reference]. As such, a model with explicit memory such as a Neural Turing Machine (NTM) [Reference] or the more modern Differentiable Neural Computer (DNC) [Reference] could be of interest here in order to generate the repeated structures that are often found in audio data. We theorize that it could be possible for a typical GAN network, such as the one we have presented here, to generate the base waveforms required for the audio signal. These structures could then be stored in explicit memory inside a DNCs or NTMs memory, and written out by the controller network at the appropriate locations in the output signal. We prefer a GAN approach to training over explicitly modelling the loss as the error on the training signal, as GANs are able to generate individual samples from a plausible set, while explicit loss models are indecisive and tend to generate the average over a set of plausible samples [GAN super resolution paper reference]. For images, this results in the typical blurry output seen in many generative models. Applications of the GAN model to recurrent architectures has been relatively little explored, and we believe this would be an interesting area for future research.</div><div></div><div>&nbsp;[To check: keyboard typing - gaps between audio events]. </div><div></div><div>Our model also has trouble generating layered sound effects. The presence of layered signals in audio data presents an interesting distinction from the image data that GANs are typically trained on. Image data typically contains less layered signals than audio data. Although image data can contain layered signals where the image contains reflections and transparency, there are typically only two or three signal layers in these cases, whereas audio data typically contains many overlapping signals. Sound effects especially, are typically built up by layering different sounds on top of one another to produce rich and expressive audio signals. We find that our generative model has a tendency to produce somewhat muddy audio signals, which do not contain clearly distinguishable audio layers. An interesting area of future research would be to create a network that can produce a varying number of outputs, with each output corresponding to a separate audio source signal.     These audio signals could then be combined to produce the final output signal.    Again, we could use a NTM or DNC network to handle the varying number of output signals.  In order to train a network like this, we would first need to perform blind signal separation on the training data in order to separate out individual source signals from the original mixed signals. We could then train a network to generate these source signal sets, instead of the original mixed signal.</div><div></div><div>In this research we relied on human judges to determine the quality of our model. However, during development it is difficult, time consuming and expensive to use human judges to evaluate the performance of different models against one another. Therefore, it is desirable to have a method of automatic evaluation which correlates well with human evaluation. Probably the most common automatic metric for generative models is the inception score, which has been shown to correlate with human judgment[Reference]. However, this model has recently been superseded by a new metric known as the Fr√©chet Inception Distance [Reference], which compares statistics of the generated signals against those of the real signals, instead of evaluating generated signals in isolation. Both these evaluation metrics use coding layers from publicly available pre-trained inception classifier networks in order to calculate their respective metrics and insure consistency evaluation across different models. The inception networks used by these automatic metric algorithms are typically trained to classify imagenet images. Unfortunately there is no corresponding inception network trained&nbsp;</div>