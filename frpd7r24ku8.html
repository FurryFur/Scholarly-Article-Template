<h1 data-label="852589" class="ltx_title_section">Evaluation</h1><div>We evaluate our model using a pane of [number of evaluators] human judges. For each judge, we draw ten random examples from the generator a ask</div><div>[In batches of ten random examples,  we ask annotators to label which digit they perceive in each example, and compute their accuracy  with respect to the classifierâ€™s labels (random accuracy would be 10%). After the ten questions, each  annotator is asked to assign subjective values of 1 through 5 for criteria of sound quality, ease of  intelligibility, and speaker diversity. ]</div>