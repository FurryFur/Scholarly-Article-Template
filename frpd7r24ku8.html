<h1 data-label="852589" class="ltx_title_section">Evaluation</h1><div>We evaluate our model using a panel of five human judges. Evaluation covers five different criteria, sound effect interpretability, sound effect realism, sound effect diversity, condition matching and overall sound effect quality. We use a model trained on the Magic Elements dataset for our evaluation. This dataset has many distinct modalities, while also providing a limiting context for sound effect generation. This allows judges to quickly understand what kind of sound effects can be generated so as to limit conditioning text queries to be in line with the generators knowledge. It also helps judges interpret sound effects when visual feedback is lacking by narrowing the scope of sound effects to the category of Magic.</div><div></div><div>Our     interpretability&nbsp;test is designed to evaluate whether the generator is able to produce sounds that are immediately recognizable. This is an important quality for sound effects, as they are used, along with visual feedback, to sell a particular action in game. If the sound effects are not immediately recognizable, they may not be able to adequately sell an effect and may instead create a disconnect between between what the player is hearing, and what is happening in game. To evaluate     interpretability, we first establish a baseline for interpretability of the real sound effects. To do this, for each judge, we draw ten sound effects from the dataset at random. Each judge then labels each sound effect with what spell, or what general magic related action, they think the sound effect represents. Then, for each judge, we draw another ten random samples from the generator, and ask the judges to perform the same lavb. Conditioning text for the generator are selected in the same manner as described above (<span class="au-ref raw v1">\ref{151756}</span>) for the training process. Afterwards, judges are shown the true  conditioning text used to generate the sample. Each judge then decides whether they correctly labeled each sample or not. We leave this decision up to the judges, as perceived correctness is more important than exact matches when it comes to interpretability.&nbsp;</div><div></div><div></div><div></div>