<h1 data-label="852589" class="ltx_title_section">Evaluation</h1><div>We evaluate our model using a panel of [number of evaluators] human judges. Evaluation covers five different criteria, sound effect interpretability, sound effect realism, sound effect diversity, condition matching and overall sound effect quality. We use a model trained on the Magic Elements dataset for our evaluation. This dataset has many distinct modalities, while also providing a limiting context for sound effect generation. This allows judges to quickly understand what kind of sound effects&nbsp;</div><div></div><div>Our     interpretability&nbsp;test is designed to evaluate whether the generator is able to produce sounds that are immediately recognizable. This is an important quality for sound effects, as they are used, along with special effects, to sell a particular action in game. If the sound effects are not immediately recognizable, they may not be able to adequately sell an effect and may instead create a disconnect between between what the player is hearing, and what is happening in game. To evaluate     interpretability, for each judge, we draw ten random samples from the generator and ask the judges to label each example with their perceived interpretation of what the sound effect represents. Conditioning text for the generator are selected in the same manner as described above (<span class="au-ref raw v1">\ref{151756}</span>) for the training process. Afterwards, judges are shown the true  conditioning text used to generate the sample. Each judge then decides whether they correctly labeled each sample or not. We leave this decision up to the judges, as perceived correctness is more important than exact matches when it comes to interpretability.&nbsp;</div><div></div><div></div><div></div>